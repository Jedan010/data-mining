{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关联规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关联规则挖掘是指从事务数据库，关系数据库和其他信息存储中的大量数据的项集之间发现有趣的、频繁出现的模式、关联和相关性。关联规则被广泛应用在购物篮分析、分类设计、捆绑销售和亏本销售分析之中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念\n",
    "给定：\n",
    "- 项的集合： $I=\\left \\{i_1,i_2,...,i_n\\right \\}$\n",
    "- 任务相关数据 $D $是数据库事务的集合，每个事务 $T $则是项的集合，使得 $T \\subseteq I$\n",
    "- 每个事务由事务标识符 $TID$ 标识；\n",
    "- $A,B$ 为两个项集，事务 $T$ 包含 $A$ 当且仅当 $A \\subseteq T$\n",
    "\n",
    "则关联规则是如下蕴涵式：\n",
    "\n",
    "$$ A \\Rightarrow  B [s,c]  \\tag{1}$$\n",
    "\n",
    "其中 $A \\subseteq I, B\\subseteq I$ 并且 $A \\bigcap B = \\varnothing $  ，规则 $A \\Rightarrow B$ 在事务集 $D$ 中成立，并且具有支持度 $s$ 和置信度 $c$ 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规则度量：支持度和置信度\n",
    "\n",
    "对所有满足最小支持度和置信度的关联规则：\n",
    " - 支持度 $s$ 是指事务集 $D$ 中包含 $ A\\cup B$ 的百分比: \n",
    " \n",
    " $$ support( A \\Rightarrow B) = P(A \\cup B) \\tag{2} $$\n",
    " \n",
    " - 置信度 $c$ 是指 $D$ 中包含 $A$ 的事务同时也包含 $B$ 的百分比：\n",
    " \n",
    " $$ confidence( A \\Rightarrow B) = P(B|A) = \\frac{P(A \\cup B)}{P(A)} \\tag{3} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如：\n",
    "\n",
    "|  TID | 购买的item |\n",
    "|:----:|:----------|\n",
    "| 2000 |    A,B,C   |\n",
    "| 1000 |     A,C    |\n",
    "| 4000 |     A,D    |\n",
    "| 5000 |    B,E,F   |\n",
    "\n",
    "则 $A$ 与 $C$ 的关联规则 为：\n",
    "- $A \\Rightarrow  C(50\\%, 66.6\\%)$\n",
    "- $C \\Rightarrow  A(50\\%, 100\\%)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关联规则挖掘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本概念\n",
    "\n",
    "- $k－项集$：包含 $k$ 个项的集合\n",
    "    - {牛奶，面包，黄油} 是个$3－项集$\n",
    "- 项集的频率是指包含项集的事务数\n",
    "- 如果项集的频率大于（最小支持度 $×$ $D$中的事务总数），则称该项集为 **频繁项集** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关联规则挖掘包含两个过程：\n",
    "- 找出所有频繁项集\n",
    "    - 大部分的计算都集中在这一步\n",
    "- 由频繁项集产生强关联规则\n",
    "    - 即满足最小支持度和最小置信度的规则\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori 算法基本知识\n",
    "\n",
    "Apriori是关联规则模型中的经典算法。它算法利用频繁项集性质的先验知识（prior knowledge），通过逐层搜索的迭代方法，即将 $k-项集$ 用于探察 $(k+1)-项集$ ，来穷尽数据集中的所有频繁项集。\n",
    "> 先找到频繁 $1-项集$ 集合 $L_1$ ,然后用 $L_1$ 找到频繁 $2-项集$ 集合 $L_2$ ，接着用 $L_2$找 $L_3$ ，直到找不到频繁 $k-项集$ ，找每个 $L_k$ 需要一次数据库扫描。\n",
    "\n",
    "Apriori性质：**频繁项集的所有非空子集也必须是频繁的。（ $A \\cup B$ 模式不可能比 $A$ 更频繁的出现）**\n",
    "\n",
    "> Apriori算法是反单调的，即一个集合如果不能通过测试，则该集合的所有超集也不能通过相同的测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori算法步骤\n",
    "Apriori算法由连接和剪枝两个步骤组成。\n",
    "\n",
    " - **连接**：为了找 $L_k$，通过 $L_{k-1}$ 与自己连接产生候选 $k-项集$ 的集合，该候选 $k$ 项集记为 $C_k$。\n",
    "    - $L_{k-1}$ 中的两个元素 $L_1$ 和 $L_2$ 可以执行连接操作 $l_1 \\triangleright  \\triangleleft l_2$ 的条件是：\n",
    "         $$ (l_1[1] = l_2[1]) \\wedge (l_1[2] = l_2[2]) \\wedge \\dots \\wedge (l_1[k-2] = l_2[k-2]) \\wedge (l_1[k-1] = l_2[k-1]) \\tag{4}$$\n",
    "         \n",
    "         \n",
    " - **剪枝**： $C_k$ 是 $L_k$ 的超集，即它的成员可能不是频繁的，但是所有频繁的 $k-项集$ 都在 $C_k$ 中。因此可以通过扫描数据库，通过计算每个 $k-项集$ 的支持度来得到 $L_k$ 。\n",
    "> 为了减少计算量，可以使用 Apriori 性质，即如果一个 $k-项集$ 的 $(k-1)-子集$ 不在 $L_{k-1}$ 中，则该候选不可能是频繁的，可以直接从 $C_{k}$ 删除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori算法示例：\n",
    "![png](./fig/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用Apiori性质由L2产生C3\n",
    " 1. 连接：C3=L2 $\\triangleright \\triangleleft$ L2 =[{A,C},{B,C},{B,E}{C,E}] $\\triangleright \\triangleleft$ [{A,C},{B,C},{B,E}{C,E}] = [{A,B,C},{A,C,E},{B,C,E}]\n",
    " 2. 使用Apriori性质剪枝：频繁项集的所有子集必须是频繁的，对候选项C3，我们可以删除其子集为非频繁的选项：\n",
    "    - {A,B,C}的2项子集是{A,B},{A,C},{B,C}，其中{A,B}不是L2的元素，所以删除这个选项；\n",
    "    - {A,C,E}的2项子集是{A,C},{A,E},{C,E}，其中{A,E} 不是L2的元素，所以删除这个选项；\n",
    "    - {B,C,E}的2项子集是{B,C},{B,E},{C,E}，它的所有2－项子集都是L2的元素，因此保留这个选项。\n",
    " 3. 这样，剪枝后得到C3=[{B,C,E}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由频繁项集产生关联规则\n",
    "\n",
    "同时满足最小支持度和最小置信度的才是强关联规则，从频繁项集产生的规则都满足支持度要求，而其置信度则可由一下公式计算：\n",
    "\n",
    "$$ confidence(A \\Rightarrow B) = P(B|A) = \\frac{support_{count}(A\\cup B)}{support_{count}(A)} \\tag{5} $$\n",
    "\n",
    "每个关联规则可由如下过程产生： \n",
    " - 对于每个频繁项集 $l$，产生 $l$ 的所有非空子集\n",
    " \n",
    " \n",
    " - 对于每个非空子集 $s$，如果 $\\frac{support_{count} (A\\cup B)}{support_{count}(A)} > min_{conf}$ 则输出规则： $s \\Rightarrow (l-s)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori 算法调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RelationRecord(items=frozenset({'l2'}), support=0.7777777777777778, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'l2'}), confidence=0.7777777777777778, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'l5', 'l1'}), support=0.2222222222222222, ordered_statistics=[OrderedStatistic(items_base=frozenset({'l5'}), items_add=frozenset({'l1'}), confidence=1.0, lift=1.5)]),\n",
       " RelationRecord(items=frozenset({'l4', 'l2'}), support=0.2222222222222222, ordered_statistics=[OrderedStatistic(items_base=frozenset({'l4'}), items_add=frozenset({'l2'}), confidence=1.0, lift=1.2857142857142856)]),\n",
       " RelationRecord(items=frozenset({'l5', 'l2'}), support=0.2222222222222222, ordered_statistics=[OrderedStatistic(items_base=frozenset({'l5'}), items_add=frozenset({'l2'}), confidence=1.0, lift=1.2857142857142856)]),\n",
       " RelationRecord(items=frozenset({'l5', 'l2', 'l1'}), support=0.2222222222222222, ordered_statistics=[OrderedStatistic(items_base=frozenset({'l5', 'l1'}), items_add=frozenset({'l2'}), confidence=1.0, lift=1.2857142857142856), OrderedStatistic(items_base=frozenset({'l5', 'l2'}), items_add=frozenset({'l1'}), confidence=1.0, lift=1.5)])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apyori import apriori ##需要安装 pip install apyori\n",
    "## test 1:\n",
    "data1 = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'],\n",
    "        ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
    "        ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
    "\n",
    "min_support = 0.22\n",
    "min_confidence = 0.7\n",
    "\n",
    "list(apriori(data1, min_support=min_support, min_confidence=min_confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['m', 'o', 'n', 'k', 'e', 'y'],\n",
       " ['d', 'o', 'n', 'k', 'e', 'y'],\n",
       " ['m', 'a', 'k', 'e', '', ''],\n",
       " ['m', 'u', 'c', 'k', 'y', ''],\n",
       " ['c', 'o', 'o', 'k', 'i', 'e']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = [['m', 'o', 'n', 'k', 'e', 'y' ],\n",
    "    ['d', 'o', 'n', 'k', 'e', 'y' ],\n",
    "    ['m', 'a', 'k', 'e','',''],\n",
    "    ['m', 'u', 'c', 'k', 'y',''],\n",
    "    ['c', 'o', 'o', 'k', 'i', 'e']]\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RelationRecord(items=frozenset({'e'}), support=0.8, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'e'}), confidence=0.8, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'k'}), support=1.0, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'k'}), confidence=1.0, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'e', 'k'}), support=0.8, ordered_statistics=[OrderedStatistic(items_base=frozenset({'e'}), items_add=frozenset({'k'}), confidence=1.0, lift=1.0), OrderedStatistic(items_base=frozenset({'k'}), items_add=frozenset({'e'}), confidence=0.8, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'e', 'o'}), support=0.6, ordered_statistics=[OrderedStatistic(items_base=frozenset({'o'}), items_add=frozenset({'e'}), confidence=1.0, lift=1.25)]),\n",
       " RelationRecord(items=frozenset({'k', 'm'}), support=0.6, ordered_statistics=[OrderedStatistic(items_base=frozenset({'m'}), items_add=frozenset({'k'}), confidence=1.0, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'k', 'o'}), support=0.6, ordered_statistics=[OrderedStatistic(items_base=frozenset({'o'}), items_add=frozenset({'k'}), confidence=1.0, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'k', 'y'}), support=0.6, ordered_statistics=[OrderedStatistic(items_base=frozenset({'y'}), items_add=frozenset({'k'}), confidence=1.0, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'e', 'o', 'k'}), support=0.6, ordered_statistics=[OrderedStatistic(items_base=frozenset({'e', 'o'}), items_add=frozenset({'k'}), confidence=1.0, lift=1.0), OrderedStatistic(items_base=frozenset({'k', 'o'}), items_add=frozenset({'e'}), confidence=1.0, lift=1.25)])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(apriori(data2, min_support=0.6, min_confidence=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori 算法实现（选读）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 每部分函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['l1', 'l2', 'l5'],\n",
       " ['l2', 'l4'],\n",
       " ['l2', 'l3'],\n",
       " ['l1', 'l2', 'l4'],\n",
       " ['l1', 'l3'],\n",
       " ['l2', 'l3'],\n",
       " ['l1', 'l3'],\n",
       " ['l1', 'l2', 'l3', 'l5'],\n",
       " ['l1', 'l2', 'l3']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_support = 0.22\n",
    "data_set = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'],\n",
    "        ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
    "        ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1', 'l2', 'l3', 'l4', 'l5'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 记录得到包含的元素\n",
    "data_list = set(item for data in data_set for item in data)\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = {}\n",
    "L = {}\n",
    "L_rule = {}\n",
    "items = {}\n",
    "num = len(data_set)\n",
    "min_s = min_support * num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用于计算C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_c(data_set, data_list, i, items, C):\n",
    "    ## 将每个item 当成字典，存在 items中\n",
    "    items['item_' + str(i)] = set(itertools.combinations(data_list,i))\n",
    "    C['C_'+str(i)] = {}  \n",
    "    ## 对每个item 计算含有的个数，即support       \n",
    "    for item in items['item_' + str(i)]:\n",
    "        s = 0\n",
    "        for data in data_set:\n",
    "            ## 判断 元素是否存在 item 中，如果在则 +1\n",
    "            if set(item).issubset(data):\n",
    "                s += 1 \n",
    "        C['C_'+str(i)][item] = s\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C_1': {('l1',): 6, ('l2',): 7, ('l3',): 6, ('l4',): 2, ('l5',): 2},\n",
       " 'C_2': {('l2', 'l1'): 4,\n",
       "  ('l2', 'l4'): 2,\n",
       "  ('l3', 'l1'): 4,\n",
       "  ('l3', 'l2'): 4,\n",
       "  ('l3', 'l4'): 0,\n",
       "  ('l3', 'l5'): 1,\n",
       "  ('l4', 'l1'): 1,\n",
       "  ('l5', 'l1'): 2,\n",
       "  ('l5', 'l2'): 2,\n",
       "  ('l5', 'l4'): 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = get_c(data_set, data_list, 1, items, C)\n",
    "C = get_c(data_set, data_list, 2, items, C)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用于计算L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l(L, C, i, min_s):\n",
    "    L['L_'+str(i)] = {}\n",
    "    ## 对c中的每个item 判断是否大于最小的支持个数，大于的存在L中\n",
    "    for item in C['C_'+str(i)]:\n",
    "        if C['C_'+str(i)][item] >= min_s:\n",
    "            L['L_'+str(i)][item] = C['C_'+str(i)][item]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L_1': {('l1',): 6, ('l2',): 7, ('l3',): 6, ('l4',): 2, ('l5',): 2},\n",
       " 'L_2': {('l2', 'l1'): 4,\n",
       "  ('l2', 'l4'): 2,\n",
       "  ('l3', 'l1'): 4,\n",
       "  ('l3', 'l2'): 4,\n",
       "  ('l5', 'l1'): 2,\n",
       "  ('l5', 'l2'): 2}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = get_l(L, C, i=1, min_s=min_s)\n",
    "L = get_l(L, C, i=2, min_s=min_s)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用于计算强相关规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relu(L_rule, L, i, num):\n",
    "    L_rule['L_'+str(i)] = []\n",
    "    ## 遍历频繁项级 L 中的每个元素\n",
    "    for m in L['L_'+str(i)]:\n",
    "        for n in range(1,i):\n",
    "            # 分别对每个元素进行组合\n",
    "            for j in itertools.combinations(m,n):\n",
    "                A = j\n",
    "                B = set(m) - set(A)      ## B 为 A 在 L 中的补集 \n",
    "\n",
    "                ## 分别计算 A 在频繁项级中的个数 和 B 在频繁项级中的个数\n",
    "                A_num = L['L_'+str(len(A))].get(tuple(A)) \n",
    "                B_num = L['L_'+str(len(m))][m]\n",
    "\n",
    "                ## 计算置信度和支持度\n",
    "                support = A_num / num \n",
    "                confidence = B_num / A_num  ## 根据公式(5)计算置信度\n",
    "\n",
    "                ## 判断是置信度知否大于最小置信度\n",
    "                if confidence >= min_confidence:\n",
    "                    L_rule['L_'+str(i)].append([set(A) , B, str(round(support, 2)), str(round(confidence, 2))])\n",
    "    return L_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L_2': [[{'l4'}, {'l2'}, '0.22', '1.0'],\n",
       "  [{'l5'}, {'l1'}, '0.22', '1.0'],\n",
       "  [{'l5'}, {'l2'}, '0.22', '1.0']]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_rule = get_relu(L_rule,L, 2, num)\n",
    "L_rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 打印强关联规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_rule(L_rule):\n",
    "    for key, value in L_rule.items():\n",
    "        print(key)\n",
    "        for item in value:\n",
    "            print(item[0] , '==>' , item[1], end='\\t\\t')\n",
    "            print('support: %.2f' %float(item[2]), end='\\t\\t')\n",
    "            print('confidence: %.2f' %float(item[3]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_2\n",
      "{'l4'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5'} ==> {'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_rule(L_rule=L_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 总的 Apriori 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(data_set, min_support, min_confidence,print_result=False):\n",
    "    \n",
    "    num = len(data_set)\n",
    "    min_s = min_support * num   \n",
    "    \n",
    "    items = {}\n",
    "    C = {}\n",
    "    L = {}\n",
    "    L_rule = {}\n",
    "    \n",
    "    data_list = set(item for data in data_set for item in data)\n",
    "    \n",
    "    for i in range(1, num+1):\n",
    "        C = get_c(data_set, data_list, i, items, C)\n",
    "        L = get_l(L, C, i, min_s=min_s)\n",
    "        \n",
    "        ## 用于终止频繁项集的迭代\n",
    "        if not L['L_'+str(i)]:\n",
    "            break\n",
    "            \n",
    "        L_rule = get_relu(L_rule,L, i, num)\n",
    "    \n",
    "    L_rule.pop('L_1')\n",
    "    if print_result:\n",
    "        print_rule(L_rule)\n",
    "\n",
    "    \n",
    "    return L_rule, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_2\n",
      "{'l4'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5'} ==> {'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "\n",
      "L_3\n",
      "{'l5'} ==> {'l2', 'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5', 'l2'} ==> {'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5', 'l1'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test 1:\n",
    "data = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'],\n",
    "        ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
    "        ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
    "\n",
    "min_support = 0.22\n",
    "min_confidence = 0.7\n",
    "rule, l = apriori(data, min_support, min_confidence, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_2\n",
      "{'o'} ==> {'e'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "{'y'} ==> {'k'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "{'o'} ==> {'k'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "{'k'} ==> {'e'}\t\tsupport: 1.00\t\tconfidence: 0.80\n",
      "{'e'} ==> {'k'}\t\tsupport: 0.80\t\tconfidence: 1.00\n",
      "{'m'} ==> {'k'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "\n",
      "L_3\n",
      "{'o'} ==> {'k', 'e'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "{'k', 'o'} ==> {'e'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "{'e', 'o'} ==> {'k'}\t\tsupport: 0.60\t\tconfidence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test 2:\n",
    "data_set = ['monkey','donkey', 'make', 'mucky', 'cookie']\n",
    "data = [[_ for _ in d] for d in data_set]\n",
    "min_support = 0.6\n",
    "min_confidence = 0.8\n",
    "\n",
    "rule, l = apriori(data, min_support, min_confidence, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori 算法整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apriori():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.items = {}\n",
    "        self.C = {}\n",
    "        self.L = {}\n",
    "        self.L_rule = {}\n",
    "    \n",
    "    import itertools\n",
    "    def get_c(self, i):\n",
    "        ## 将每个item 当成字典，存在 items中\n",
    "        self.items['item_' + str(i)] = set(itertools.combinations(self.data_list,i))\n",
    "        self.C['C_'+str(i)] = {}  \n",
    "        ## 对每个item 计算含有的个数，即support       \n",
    "        for item in self.items['item_' + str(i)]:\n",
    "            s = 0\n",
    "            for data in self.data_set:\n",
    "                ## 判断 元素是否存在 item 中，如果在则 +1\n",
    "                if set(item).issubset(data):\n",
    "                    s += 1 \n",
    "            self.C['C_'+str(i)][item] = s\n",
    "            \n",
    "    def get_l(self, i):\n",
    "        self.L['L_'+str(i)] = {}\n",
    "        ## 对c中的每个item 判断是否大于最小的支持个数，大于的存在L中\n",
    "        for item in self.C['C_'+str(i)]:\n",
    "            if self.C['C_'+str(i)][item] >= self.min_s:\n",
    "                self.L['L_'+str(i)][item] = self.C['C_'+str(i)][item]\n",
    "                \n",
    "    def get_relu(self, i):\n",
    "        self.L_rule['L_'+str(i)] = []\n",
    "        ## 遍历频繁项级 L 中的每个元素\n",
    "        for m in self.L['L_'+str(i)]:\n",
    "            for n in range(1,i):\n",
    "                # 分别对每个元素进行组合\n",
    "                for j in itertools.combinations(m,n):\n",
    "                    A = j\n",
    "                    B = set(m) - set(A)      ## B 为 A 在 L 中的补集 \n",
    "\n",
    "                    ## 分别计算 A 在频繁项级中的个数 和 B 在频繁项级中的个数\n",
    "                    A_num = self.L['L_'+str(len(A))].get(tuple(A)) \n",
    "                    B_num = self.L['L_'+str(len(m))][m]\n",
    "\n",
    "                    ## 计算置信度和支持度\n",
    "                    support = A_num / num \n",
    "                    confidence = B_num / A_num  ## 根据公式(5)计算置信度\n",
    "\n",
    "                    ## 判断是置信度知否大于最小置信度\n",
    "                    if confidence >= self.min_confidence:\n",
    "                        self.L_rule['L_'+str(i)].append([set(A) , B, str(round(support, 2)), str(round(confidence, 2))])\n",
    "\n",
    "    def print_rule(self):\n",
    "        for key, value in self.L_rule.items():\n",
    "            print(key)\n",
    "            for item in value:\n",
    "                print(item[0] , '==>' , item[1], end='\\t\\t')\n",
    "                print('support: %.2f' %float(item[2]), end='\\t\\t')\n",
    "                print('confidence: %.2f' %float(item[3]))\n",
    "                print()\n",
    "        \n",
    "    def apriori(self, data_set, min_support, min_confidence, print_result=False):\n",
    "        self.data_set = data_set\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.print_result = print_result\n",
    "        self.data_list = set(item for data in data_set for item in data)\n",
    "\n",
    "        self.num = len(data_set)\n",
    "        self.min_s = min_support * self.num  \n",
    "\n",
    "        for i in range(1, self.num+1):\n",
    "            self.get_c(i)\n",
    "            self.get_l(i)\n",
    "            ## 用于终止频繁项集的迭代\n",
    "            if not self.L['L_'+str(i)]:\n",
    "                break\n",
    "            self.get_relu( i)            \n",
    "        \n",
    "        self.L_rule.pop('L_1', None)\n",
    "\n",
    "        if print_result:\n",
    "            print_rule(self.L_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_2\n",
      "{'l4'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5'} ==> {'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "\n",
      "L_3\n",
      "{'l5'} ==> {'l2', 'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5', 'l2'} ==> {'l1'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "{'l5', 'l1'} ==> {'l2'}\t\tsupport: 0.22\t\tconfidence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test 1:\n",
    "data = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'],\n",
    "        ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
    "        ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
    "\n",
    "min_support = 0.22\n",
    "min_confidence = 0.7\n",
    "apr_1 = Apriori()\n",
    "apr_1.apriori(data, min_support, min_confidence, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_2\n",
      "{'o'} ==> {'e'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "{'y'} ==> {'k'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "{'o'} ==> {'k'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "{'k'} ==> {'e'}\t\tsupport: 0.56\t\tconfidence: 0.80\n",
      "{'e'} ==> {'k'}\t\tsupport: 0.44\t\tconfidence: 1.00\n",
      "{'m'} ==> {'k'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "\n",
      "L_3\n",
      "{'o'} ==> {'k', 'e'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "{'k', 'o'} ==> {'e'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "{'e', 'o'} ==> {'k'}\t\tsupport: 0.33\t\tconfidence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test 2:\n",
    "data_set = ['monkey','donkey', 'make', 'mucky', 'cookie']\n",
    "data = [[_ for _ in d] for d in data_set]\n",
    "min_support = 0.6\n",
    "min_confidence = 0.8\n",
    "\n",
    "apr_2 = Apriori()\n",
    "apr_2.apriori(data, min_support, min_confidence, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
